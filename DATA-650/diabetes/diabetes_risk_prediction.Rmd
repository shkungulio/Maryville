---
title: "Diabetes Risk Prediction"
author: "Seif Kungulio"
date: "07/24/2025"
output: 
  rmdformats::readthedown
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE, message = FALSE, warning = FALSE)

```

## Business Understanding
### Objective
Build a predictive model to identify individuals in Texas at 30%+ risk of 
developing diabetes using self-reported health and demographic data to support 
early intervention.


## Data Understanding
### Load Libraries
```{r}
library(tidyverse)
library(caret)
library(ggplot2)
library(corrplot)
```
### Load the data
```{r}
data <- read.csv("diabetes_health_indicators_BRFSS2015.csv")
```
### Quick dimension, structure, and summary
```{r}
dim(data)
str(data)
summary(data)
```
The dataset contains 253,680 records and 22 variables related to health, lifestyle, and demographics, with the goal of predicting diabetes diagnosis. The target variable (`Diabetes_binary`) shows only 13.9% of individuals are diabetic, highlighting a class imbalance that must be addressed. Common health issues include high blood pressure (44.3%), high cholesterol (42.4%), and a high average BMI (33.2), indicating an overweight population. While most individuals report access to healthcare (94.2%) and healthy behaviors like physical activity and fruit/vegetable intake, nearly 20% cite cost as a barrier to care.

Self-reported health metrics such as general health, mental health, and physical health reveal that although most people rate their health positively, some experience chronic issues—evident in skewed distributions and maximum values. Demographics show a largely middle-aged, moderately educated, and middle-income population. For modeling, transformations, encoding of ordinal variables, and handling of outliers and interactions will be essential to improve performance and interpretability.

### Check class balance
```{r}
table(data$Diabetes_binary)
prop.table(table(data$Diabetes_binary))
```
The class balance check for the `Diabetes_binary` variable reveals a significant imbalance: 86.1% of individuals are non-diabetic, while only 13.9% are diabetic. This disparity can bias predictive models toward the majority class, leading to poor detection of diabetic cases. To build an effective model, techniques like resampling or class weighting should be used, and evaluation metrics such as precision, recall, and AUC should be prioritized over accuracy to ensure balanced performance.

### Histogram of BMI

```{r}
ggplot(data, aes(x = BMI)) +
  geom_histogram(bins = 50, fill = "steelblue", color = "black") +
  theme_minimal() + labs(title = "Distribution of BMI")
```
The histogram reveals the distribution of Body Mass Index (BMI) across the dataset. The shape of the distribution is noticeably right-skewed, with most values concentrated in the 25–30 range, which aligns with the clinical classification of being overweight. A substantial number of individuals fall between 20 and 40 BMI, while only a few cases extend beyond 60, which may represent extreme outliers or possible data entry anomalies. The steep peak and long tail suggest the need for data transformation, such as logarithmic or square root scaling, especially if the BMI variable is used in machine learning models sensitive to skewed distributions. In summary, the dataset reflects a population with a high incidence of overweight and obesity, a factor likely relevant to diabetes prediction.

### Boxplot of Age (proxy)

```{r}
ggplot(data, aes(x = as.factor(Diabetes_binary), y = Age)) +
  geom_boxplot(fill = "orange") +
  labs(title = "Age vs Diabetes Status", x = "Diabetes", y = "Age Group")
```
The boxplot compares the age distribution (represented as age groups) between individuals with and without diabetes. From the visualization, it's clear that those with diabetes tend to be older. The median age for non-diabetic individuals appears lower than for diabetic individuals, and the overall spread (interquartile range) is wider for the non-diabetic group. This suggests greater variability in the age of people without diabetes. Conversely, the diabetic group shows a more concentrated age range with a higher median, indicating that diabetes is more prevalent among older individuals. The presence of several lower-end outliers in the diabetic group also highlights that a small subset of younger individuals are affected. Overall, age appears to be a significant factor and potentially a strong predictor of diabetes in this dataset.

Remove outliers from Age within each Diabetes group
```{r}
data <- data %>%
  group_by(Diabetes_binary) %>%
  filter(
    Age > quantile(Age, 0.25) - 1.5 * IQR(Age) &
    Age < quantile(Age, 0.75) + 1.5 * IQR(Age)
  ) %>%
  ungroup()
```
The code removes outliers from the `Age` variable using the IQR method, applied separately within each diabetes group (`Diabetes_binary`). This group-wise filtering ensures that extreme age values are excluded based on the distribution of each group, resulting in a cleaner dataset (`data`) for more accurate analysis, visualizations, and modeling.

### Correlation among numeric variables
```{r}
numeric_vars <- data %>% select(BMI, MentHlth, PhysHlth, Age)
corrplot(cor(numeric_vars), method = "color", type = "upper")
```
The correlation matrix provides insight into the linear relationships among four numeric variables: BMI, Mental Health (MentHlth), Physical Health (PhysHlth), and Age. The results show that there is minimal correlation between these variables. For instance, BMI shows very weak correlations with both mental and physical health days, and age exhibits only a modest positive correlation with physical health. This lack of strong multicollinearity suggests that these variables contribute independent information to the dataset and are not redundant. As such, each of them may be valuable in a predictive model for diabetes. However, the low pairwise correlations also indicate that any potential interactions or nonlinear effects should be explored through feature engineering or more complex modeling approaches.


## Data Preparation
### Handle duplicates
```{r}
data <- distinct(data)
dim(data)
```
The output shows that duplicate records were removed from the dataset using the distinct(data) function from the dplyr package in R. This function retains only unique rows by eliminating exact duplicates across all columns. Following this step, the dimensions of the dataset changed from 253,680 to 229,474 rows, indicating that 24,206 duplicate entries were found and removed.

### Binning Mental and Physical Health
```{r}
data$MentHlth_binned <- cut(data$MentHlth,
                            breaks = c(-1, 0, 10, 20, 30),
                            labels = c("None", "Low", "Moderate", "High"))
data$PhysHlth_binned <- cut(data$PhysHlth,
                            breaks = c(-1, 0, 10, 20, 30),
                            labels = c("None", "Low", "Moderate", "High"))
```
The code bins the `MentHlth` and `PhysHlth` variables—representing the number of unhealthy days—into four categorical levels: "None," "Low," "Moderate," and "High." This simplifies analysis, improves interpretability, and supports visualizations or modeling, especially for skewed data. The binning enhances clarity but should be evaluated to ensure meaningful thresholds and balanced group sizes.

### Scale BMI
```{r}
data$BMI_scaled <- scale(data$BMI)
```

### Age Groups
```{r}
data$Age_group <- cut(data$Age,
                      breaks = c(0, 4, 8, 13),
                      labels = c("18–34", "35–54", "55+"))
```

### Feature Engineering: Chronic Risk Load
```{r}
data$Chronic_Risk_Load <- with(data, HighBP + HighChol + Stroke + HeartDiseaseorAttack)
```
The `Chronic_Risk_Load` variable is a newly engineered feature that sums four binary health indicators—high blood pressure, high cholesterol, stroke, and heart disease—to reflect an individual’s total chronic disease burden. Ranging from 0 to 4, it provides a simplified, interpretable measure of health risk that can enhance predictive modeling and population stratification. This feature helps capture the compounded effect of comorbidities and supports deeper analysis of how chronic conditions relate to diabetes and other health outcomes.

### Feature Engineering: Healthcare Barrier Index
```{r}
data$Healthcare_Barrier_Index <- with(data, NoDocbcCost + (1 - AnyHealthcare))
```
The `Healthcare_Barrier_Index` is a new feature that combines lack of insurance and inability to afford doctor visits into a single score ranging from 0 to 2. It captures the extent of healthcare access barriers, with higher values indicating greater difficulty in obtaining care. This index enhances analysis by incorporating structural health risk factors and can be used to examine how access issues relate to health outcomes like diabetes.

### Drop or convert unused columns
```{r}
data_prep <- data %>%
  select(Diabetes_binary, BMI_scaled, MentHlth_binned, PhysHlth_binned, Age_group,
         Chronic_Risk_Load, Healthcare_Barrier_Index, GenHlth, Education, Income,
         Smoker, PhysActivity, DiffWalk, HvyAlcoholConsump, Sex)
```
### Convert categorical to factor
```{r}
data_prep <- data_prep %>%
  mutate(across(where(is.character), as.factor),
         across(c(Diabetes_binary, GenHlth, Education, Income, Smoker, PhysActivity,
                  DiffWalk, HvyAlcoholConsump, Sex, Age_group,
                  MentHlth_binned, PhysHlth_binned), as.factor))
```


## Modeling: Logistic Regression (Baseline)
### Split data
```{r}
set.seed(123)
train_idx <- createDataPartition(data_prep$Diabetes_binary, p = 0.8, list = FALSE)
train <- data_prep[train_idx, ]
test <- data_prep[-train_idx, ]
```

### Logistic Regression Model
```{r}
model_logit <- glm(Diabetes_binary ~ ., data = train, family = binomial)
```

### Model Summary
```{r}
summary(model_logit)
```

### Predict probabilities and classes
```{r}
test_probs <- predict(model_logit, newdata = test, type = "response")
test_pred <- ifelse(test_probs > 0.5, 1, 0)
```

### Confusion Matrix
```{r}
confusionMatrix(as.factor(test_pred), test$Diabetes_binary, positive = "1")
```

### ROC Curve
```{r}
library(pROC)
roc_obj <- roc(test$Diabetes_binary, test_probs)
plot(roc_obj, main = "ROC Curve - Logistic Regression")
auc(roc_obj)
```


## Evaluation
### Baseline model performance evaluation
Key metrics:

* Accuracy
* Sensitivity (Recall)
* Specificity
* AUC (already calculated above)

### Variable importance
```{r}
exp(coef(model_logit))
```
