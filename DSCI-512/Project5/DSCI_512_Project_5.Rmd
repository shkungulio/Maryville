---
title: "Tree Based Methods"
author: "by Seif Kungulio"
output:
  html_document:
    theme: cosmo
    toc: true
    toc_depth: 4
    toc_title: "Table of Contents"
    toc_float:
      collapsed: true
      smooth_scroll: false
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```

<hr style="height:5px; border:none; color:red; background-color:red;" />

<br>

## **Bike Sharing System**
<hr style="height:2px; border:none; color:#333; background-color:#333;" />

You are working as a data scientist for the government in the city of Washington, D.C. Currently, Washington, D.C has a bike sharing system. People could rent a bike from one location and return it to a different place. You are given a historical usage pattern with weather data contained in the Excel workbook bike.csv. You are asked to forecast bike rental demand in the capital bike share program.

**Data Source:** from Kaggle at https://www.kaggle.com/c/bike-sharing-demand

<br>

### **Data Dictionary**
This dataset contains the following columns:

| **Variable** | **Description** | **Data Type** | **Rules/Constraints** |
|:-------------|:----------------|:--------------|:----------------------|
| **datetime** | Date and time of the observation | Timestamp | Must be unique and in ISO 8601 format (YYYY-MM-DD HH:MM:SS). |
| **season** | Season (1: Winter, 2: Spring, 3: Summer, 4: Fall) | Categorical | Must be an integer between 1 and 4. |
| **holiday** | Whether the day is a holiday (0: No, 1: Yes) | Binary | Must be 0 or 1. |
| **workingday** | Whether the day is a working day (0: No, 1: Yes) | Binary | Must be 0 or 1. |
| **weather** | Weather condition (1 to 4, where 1: Clear, 4: Extreme) | Categorical | Must be an integer between 1 and 4. |
| **temp** | Normalized temperature in Celsius | Continuous | Must be between 0 and 41. |
| **atemp** | Normalized "feels like" temperature in Celsius | Continuous | Must be between 0 and 45.455. |
| **humidity** | Relative humidity (%) | Continuous | Must be between 0 and 100. |
| **windspeed** | Wind speed in km/h | Continuous | Must be non-negative, and maximum value is approximately 56.9979. |
| **casual** | Number of casual (non-registered) users | Integer | Must be non-negative, with a maximum of 367. |
| **registered** | Number of registered users | Integer | Must be non-negative, with a maximum of 886. |
| **count** | Total number of bike rentals | Integer | Must equal casual + registered, and be non-negative. |
|  |  |  |  |

<br>

### **Development Environment**

Install necessary packages if not installed
```{r install-libraries}
if (!requireNamespace("tree", quietly = TRUE)) {
  install.packages("tree")
}
if (!requireNamespace("randomForest", quietly = TRUE)) {
  install.packages("randomForest")
}
```

Load the required libraries
```{r load-libraries, message=FALSE}
library(tree)
library(randomForest)
```


<br>

### **Question 1:**
Load the dataset bike.csv into memory. Then split the data into a training set containing 2/3 of the original data (test set containing remaining 1/3 of the original data).  

Read the dataset into memory
```{r}
Bike.df <- read.csv("data/Bike.csv")
```

Display the dimensions of the data frame (number of rows and columns)
```{r}
dim(Bike.df)
```

Display the column names of the data frame
```{r}
colnames(Bike.df)
```

Display the first six rows of the data frame to understand its structure
```{r}
head(Bike.df)
```

Convert categorical variables to factors
```{r}
Bike.df$season = factor(Bike.df$season,
                         levels = c(1, 2, 3, 4),
                         labels = c("Spring", "Summer", "Fall", "Winter")
)
Bike.df$holiday <- factor(Bike.df$holiday, 
                           levels = c(0,1), 
                           labels = c("No", "Yes")
)
Bike.df$workingday <- factor(Bike.df$workingday,
                              levels = c(0,1), 
                              labels = c("No", "Yes")
)
Bike.df$weather <- factor(Bike.df$weather,
                           levels = c(1, 2, 3, 4),
                           labels = c("Clear", "Misty_cloudy",
                                      "Light_snow", "Heavy_rain")
)
```

Set seed for reproducibility
```{r}
set.seed(123)
```

Split the data into a training set (2/3) and test set (1/3)
```{r}
trainIdx = sample(1:nrow(Bike.df), size = 2/3 * nrow(Bike.df))

# Create a training dataset out of "Bike.df"
trainData = Bike.df[trainIdx, ]

# Create a testing dataset from the remaining dataset of "Bike.df"
testData  <- Bike.df[-trainIdx, ]
```

<br>

### **Question 2:**
Build a tree model using function tree().

#### **Section A** 
The response is count and the predictors are season, holiday, workingday, temp, atemp, humidity, windspeed, casual, and registered.
```{r}
# Build a tree model using function tree()
tr.model = tree(
  count ~ season + holiday + workingday + temp +
    atemp + humidity + windspeed + casual + registered,
  data = trainData
)

# Display the statistical summary of the tree
summary(tr.model)
```
* The decision tree mainly depends on the registered and casual variables, which seem to be the most direct predictors of bike rental counts. 
* With 8 terminal nodes, the model reflects a moderate level of complexity while still managing to avoid overfitting.
* The Residual Mean Deviance (1875) and the distribution of residuals give insight into how well the model performs. Although there are some notable outliers (like -158 and +259.7), the residuals generally cluster around zero.
* Overall, the tree offers a simplified yet understandable view of the data relationships. However, its predictive accuracy could potentially be enhanced through hyperparameter tuning or by exploring more advanced models such as random forests or gradient boosting.  

<br>

#### **Section B**
Perform cross-validation to choose the best tree by calling cv.tree().
```{r}
CV_Bike = cv.tree(tr.model)
```

<br>

#### **Section C**
Plot the model results of b) and determine the best size of the optimal tree.
```{r}
plot(CV_Bike$size, CV_Bike$dev, type = "b",
     xlab = "Tree Size",
     ylab = "Deviance",
     main = "Cross Validation Results")
```

* The plot indicates that the ideal tree size is 4.
* At this point, the deviance reaches its lowest value before leveling off or slightly rising with larger trees. This suggests the model achieves a good balance—complex enough to capture key patterns without being overly complicated or prone to overfitting.

<br>

#### **Section D**
Prune the tree by calling prune.tree() function with the best size found in c).
```{r, fig.width = 12, fig.height = 6}
prunedTree = prune.tree(tr.model, best = 4)
```

<br>

#### **Section E**
Plot the best tree model.
```{r, fig.width = 12, fig.height = 6}
plot(prunedTree)
text(prunedTree, pretty = 0)
```

* The pruned decision tree, refined to 4 terminal nodes through cross-validation, forecasts a numeric target variable. It begins by splitting on the **`registered`** variable, with the initial split at **`registered` < 200.5**. Subsequent splits lead to four final predicted values: 40.02, 180.40, 332.80, and 553.20.
* This streamlined version of the tree is more interpretable, reduces the risk of overfitting, and maintains solid predictive power by emphasizing the most impactful decision points.

<br>

#### **Section F**
Compute the test error using the test data set.
```{r}
tr.predictions = predict(prunedTree, newdata = testData)

#compute and display the MSE
tr.mse <- mean((tr.predictions - testData$count)^2)
tr.mse
```

* The Mean Squared Error (MSE) of **`r round(tr.mse, 3)`** gives a numerical indication of the average difference between the model’s predictions and the actual values in the test dataset.
* Whether this MSE is considered good or bad depends on the scale of the target variable **`count`**.
  * For example, if **`count`** values usually fall in the thousands, this level of error might be reasonable. But if the values are much smaller, the error could be considered quite large in comparison.

<br>

```{r, echo=FALSE}
#print("End of file")
#knitr::knit_exit()
```

### **Question 3:**
Build a random forest model using function randomForest().

#### **Section A** 
The response is **`count`** and the predictors are **`season`**, **`holiday`**, **`workingday`**, **`temp`**, **`atemp`**, **`humidity`**, **`windspeed`**, **`casual`**, and **`registered`**.
```{r}
set.seed(123)  # Set seed for reproducibility
rf.model = randomForest(count ~ season + holiday + workingday + temp + 
                          atemp + humidity + windspeed + casual + registered,
                        data = trainData, importance = TRUE)

# Print the random forest model result
print(rf.model)
```

* The Random Forest model performs exceptionally well on the training data, accounting for 99.6% of the variance and showing a low residual error of 130.3723. However, this strong performance might suggest overfitting, so it's important to validate the model on unseen test data.
* Using 500 trees and 3 variables at each split, the model strikes a good balance between precision and computational efficiency.
* To determine which features, have the greatest impact on predictions, tools like **`importance()`** or **`varImpPlot()`** can be used to assess variable importance.

<br>

#### **Section B**
Compute the test error using the test data set.
```{r}
rf.predictions = predict(rf.model, newdata = testData)

# Calculate the mean squared error
rf.mse <- mean((rf.predictions - testData$count)^2)
rf.mse
```

* The test Mean Squared Error (MSE) of **`r round(rf.mse, 3)`** reflects strong predictive accuracy on new data, although it's slightly higher than the training error of 130.3723—indicating a minor degree of overfitting.
* Overall, the model demonstrates solid performance, with only a small drop in accuracy from training to testing, which is a normal and expected outcome in most modeling scenarios.

<br>

#### **Section C**
Extract variable importance measure using **`importance()`** function.
```{r}
importance(rf.model)
```

* The variable **`registered`** plays the most significant role in predicting the target variable **`count`**, with **`casual`** and **`workingday`** also contributing notably.
* In contrast, features such as **`holiday`** and **`windspeed`** have little influence on the model’s predictive capability.
* These findings can guide model simplification by removing less impact variables and highlight the main drivers behind the predictions.

<br>

#### **Section D**
Plot the variable importance using function **`varImpPlot()`**. Which are the top 2 important predictors in this model?
```{r, fig.width = 12, fig.height = 6}
varImpPlot(rf.model)
```

* **`Registered`** and **`casual`** are the top predictors, playing a major role in improving the model's accuracy and reducing variance.
* The model relies heavily on these two variables, indicating they are the primary factors affecting the target variable, **`count`**.
