# Load libraries
library(MASS)
library(caret)
library(tree)
library(randomForest)
library(e1071)
library(cluster)
library(factoextra)
library(dplyr)
library(nnet)

# 1. Data Preparation
insurance <- read.csv("/mnt/data/insurance.csv")
insurance$charges <- log(insurance$charges)

# Dummy variables
dummy_data <- model.matrix(charges ~ ., data = insurance)
stopifnot(all(dummy_data[, 1] == 1))
dummy_data <- dummy_data[, -1]

# Split data
set.seed(1)
n <- nrow(insurance)
train_idx <- sample(1:n, floor(2/3 * n))
test_idx <- setdiff(1:n, train_idx)

train_data <- insurance[train_idx, ]
test_data <- insurance[test_idx, ]

train_dummy <- dummy_data[train_idx, ]
test_dummy <- dummy_data[test_idx, ]

# 2. Multiple Linear Regression
lm_model <- lm(charges ~ age + sex + bmi + children + smoker + region, data = train_data)
summary(lm_model)

# 2.d Best subset with AIC
step_model <- stepAIC(lm_model, direction = "backward")

# 2.e LOOCV
ctrl_loocv <- trainControl(method = "LOOCV")
model_loocv <- train(charges ~ age + bmi + children + smoker + region, data = train_data,
                     method = "lm", trControl = ctrl_loocv)
loocv_mse <- model_loocv$results$RMSE^2

# 2.f 10-fold CV
ctrl_10cv <- trainControl(method = "cv", number = 10)
model_10cv <- train(charges ~ age + bmi + children + smoker + region, data = train_data,
                    method = "lm", trControl = ctrl_10cv)
tenfold_mse <- model_10cv$results$RMSE^2

# 2.g Test MSE
pred_test <- predict(step_model, newdata = test_data)
test_mse <- mean((test_data$charges - pred_test)^2)

# 3. Regression Tree
reg_tree <- tree(charges ~ age + sex + bmi + children + smoker + region, data = train_data)
cv_tree <- cv.tree(reg_tree)
best_size <- which.min(cv_tree$dev)
plot(cv_tree$size, cv_tree$dev, type = "b")

# Prune tree
pruned_tree <- prune.tree(reg_tree, best = cv_tree$size[best_size])
plot(pruned_tree)
text(pruned_tree, pretty = 0)

# Test MSE for tree
pred_tree <- predict(pruned_tree, newdata = test_data)
tree_mse <- mean((test_data$charges - pred_tree)^2)

# 4. Random Forest
rf_model <- randomForest(charges ~ age + sex + bmi + children + smoker + region, data = train_data)
rf_pred <- predict(rf_model, newdata = test_data)
rf_mse <- mean((test_data$charges - rf_pred)^2)
importance(rf_model)
varImpPlot(rf_model)

# 5. SVM
svm_model <- svm(charges ~ age + sex + bmi + children + smoker + region, data = train_data,
                 kernel = "radial", gamma = 5, cost = 50)

# Grid Search
svm_tune <- tune(svm, charges ~ age + sex + bmi + children + smoker + region, data = train_data,
                 kernel = c("linear", "polynomial", "radial", "sigmoid"),
                 ranges = list(cost = c(1, 10, 50, 100), gamma = c(1, 3, 5)))
best_svm <- svm_tune$best.model

# Predict and MSE
svm_pred <- predict(best_svm, newdata = test_data)
svm_mse <- mean((test_data$charges - svm_pred)^2)

# 6. K-Means Clustering
train_scaled <- scale(train_dummy)
train_scaled_df <- as.data.frame(train_scaled)

gap_stat <- clusGap(train_scaled_df, FUN = kmeans, nstart = 25, K.max = 10, B = 50)
fviz_gap_stat(gap_stat)
opt_k <- which.max(gap_stat$Tab[, "gap"])

km <- kmeans(train_scaled_df, centers = opt_k, nstart = 25)
fviz_cluster(km, data = train_scaled_df, geom = "point")

# 7. Neural Network
nn_model <- nnet(train_dummy, train_data$charges, size = 1, linout = TRUE)
plotnet <- function(mod) plot(mod)  # For use in RStudio

nn_pred <- predict(nn_model, newdata = test_dummy)
nn_mse <- mean((test_data$charges - nn_pred)^2)

# 8.a Compare Models
model_results <- data.frame(
  Model.Type = c("Multiple Linear Regression", "Regression Tree", "Random Forest", "Support Vector Machine", "Neural Network"),
  Test.MSE = round(c(test_mse, tree_mse, rf_mse, svm_mse, nn_mse), 4)
)
print(model_results)

# 8.b Recommend a model for sales
# Tree model is interpretable

# 8.c Reverse log on pruned tree and replot
copy_tree <- pruned_tree
copy_tree$frame$yval <- exp(copy_tree$frame$yval)
plot(copy_tree)
text(copy_tree, pretty = 0)
